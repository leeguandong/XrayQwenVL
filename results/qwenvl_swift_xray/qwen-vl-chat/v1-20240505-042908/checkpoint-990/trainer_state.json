{
  "best_metric": 1.04416406,
  "best_model_checkpoint": "/home/image_team/image_team_docker_home/lgd/e_commerce_lmm/results/qwenvl_swift_xray/qwen-vl-chat/v1-20240505-042908/checkpoint-990",
  "epoch": 4.981132075471698,
  "eval_steps": 1000,
  "global_step": 990,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "acc": 0.35792518,
      "epoch": 0.005031446540880503,
      "grad_norm": 3.828125,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 4.48069239,
      "step": 1
    },
    {
      "acc": 0.34041935,
      "epoch": 0.050314465408805034,
      "grad_norm": 4.09375,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 4.52559831,
      "step": 10
    },
    {
      "acc": 0.39652653,
      "epoch": 0.10062893081761007,
      "grad_norm": 1.3984375,
      "learning_rate": 6.666666666666667e-05,
      "loss": 3.36858749,
      "step": 20
    },
    {
      "acc": 0.5008389,
      "epoch": 0.1509433962264151,
      "grad_norm": 1.3515625,
      "learning_rate": 0.0001,
      "loss": 2.35984173,
      "step": 30
    },
    {
      "acc": 0.5486455,
      "epoch": 0.20125786163522014,
      "grad_norm": 1.15625,
      "learning_rate": 9.895833333333334e-05,
      "loss": 2.00753918,
      "step": 40
    },
    {
      "acc": 0.56725512,
      "epoch": 0.25157232704402516,
      "grad_norm": 1.109375,
      "learning_rate": 9.791666666666667e-05,
      "loss": 1.81973248,
      "step": 50
    },
    {
      "acc": 0.57348714,
      "epoch": 0.3018867924528302,
      "grad_norm": 1.0703125,
      "learning_rate": 9.687500000000001e-05,
      "loss": 1.79023495,
      "step": 60
    },
    {
      "acc": 0.58238773,
      "epoch": 0.3522012578616352,
      "grad_norm": 1.2109375,
      "learning_rate": 9.583333333333334e-05,
      "loss": 1.72146225,
      "step": 70
    },
    {
      "acc": 0.60350637,
      "epoch": 0.4025157232704403,
      "grad_norm": 1.3828125,
      "learning_rate": 9.479166666666666e-05,
      "loss": 1.58067446,
      "step": 80
    },
    {
      "acc": 0.59458008,
      "epoch": 0.4528301886792453,
      "grad_norm": 1.109375,
      "learning_rate": 9.375e-05,
      "loss": 1.64500809,
      "step": 90
    },
    {
      "acc": 0.60787306,
      "epoch": 0.5031446540880503,
      "grad_norm": 1.3359375,
      "learning_rate": 9.270833333333334e-05,
      "loss": 1.55661736,
      "step": 100
    },
    {
      "acc": 0.60477295,
      "epoch": 0.5534591194968553,
      "grad_norm": 1.296875,
      "learning_rate": 9.166666666666667e-05,
      "loss": 1.56103096,
      "step": 110
    },
    {
      "acc": 0.60405235,
      "epoch": 0.6037735849056604,
      "grad_norm": 1.5234375,
      "learning_rate": 9.062500000000001e-05,
      "loss": 1.55453377,
      "step": 120
    },
    {
      "acc": 0.61138291,
      "epoch": 0.6540880503144654,
      "grad_norm": 1.6171875,
      "learning_rate": 8.958333333333335e-05,
      "loss": 1.53154507,
      "step": 130
    },
    {
      "acc": 0.61493511,
      "epoch": 0.7044025157232704,
      "grad_norm": 1.5078125,
      "learning_rate": 8.854166666666667e-05,
      "loss": 1.52059917,
      "step": 140
    },
    {
      "acc": 0.6185914,
      "epoch": 0.7547169811320755,
      "grad_norm": 1.53125,
      "learning_rate": 8.75e-05,
      "loss": 1.4935092,
      "step": 150
    },
    {
      "acc": 0.62048125,
      "epoch": 0.8050314465408805,
      "grad_norm": 1.421875,
      "learning_rate": 8.645833333333334e-05,
      "loss": 1.49436073,
      "step": 160
    },
    {
      "acc": 0.62400393,
      "epoch": 0.8553459119496856,
      "grad_norm": 1.484375,
      "learning_rate": 8.541666666666666e-05,
      "loss": 1.49089794,
      "step": 170
    },
    {
      "acc": 0.63742633,
      "epoch": 0.9056603773584906,
      "grad_norm": 1.5703125,
      "learning_rate": 8.4375e-05,
      "loss": 1.40507154,
      "step": 180
    },
    {
      "acc": 0.63473172,
      "epoch": 0.9559748427672956,
      "grad_norm": 1.515625,
      "learning_rate": 8.333333333333334e-05,
      "loss": 1.39483232,
      "step": 190
    },
    {
      "acc": 0.62379017,
      "epoch": 1.0062893081761006,
      "grad_norm": 1.4296875,
      "learning_rate": 8.229166666666667e-05,
      "loss": 1.4571373,
      "step": 200
    },
    {
      "acc": 0.63763671,
      "epoch": 1.0566037735849056,
      "grad_norm": 1.671875,
      "learning_rate": 8.125000000000001e-05,
      "loss": 1.4006216,
      "step": 210
    },
    {
      "acc": 0.64623518,
      "epoch": 1.1069182389937107,
      "grad_norm": 1.6640625,
      "learning_rate": 8.020833333333334e-05,
      "loss": 1.35726681,
      "step": 220
    },
    {
      "acc": 0.64660139,
      "epoch": 1.1572327044025157,
      "grad_norm": 1.6171875,
      "learning_rate": 7.916666666666666e-05,
      "loss": 1.36130962,
      "step": 230
    },
    {
      "acc": 0.63943715,
      "epoch": 1.2075471698113207,
      "grad_norm": 1.7109375,
      "learning_rate": 7.8125e-05,
      "loss": 1.35630322,
      "step": 240
    },
    {
      "acc": 0.6509944,
      "epoch": 1.2578616352201257,
      "grad_norm": 1.8984375,
      "learning_rate": 7.708333333333334e-05,
      "loss": 1.32482462,
      "step": 250
    },
    {
      "acc": 0.65223727,
      "epoch": 1.3081761006289307,
      "grad_norm": 1.78125,
      "learning_rate": 7.604166666666667e-05,
      "loss": 1.32668304,
      "step": 260
    },
    {
      "acc": 0.63887272,
      "epoch": 1.3584905660377358,
      "grad_norm": 1.890625,
      "learning_rate": 7.500000000000001e-05,
      "loss": 1.35992279,
      "step": 270
    },
    {
      "acc": 0.64692817,
      "epoch": 1.408805031446541,
      "grad_norm": 1.96875,
      "learning_rate": 7.395833333333335e-05,
      "loss": 1.32854404,
      "step": 280
    },
    {
      "acc": 0.6549881,
      "epoch": 1.459119496855346,
      "grad_norm": 1.90625,
      "learning_rate": 7.291666666666667e-05,
      "loss": 1.30504303,
      "step": 290
    },
    {
      "acc": 0.65215788,
      "epoch": 1.509433962264151,
      "grad_norm": 1.8671875,
      "learning_rate": 7.1875e-05,
      "loss": 1.31133156,
      "step": 300
    },
    {
      "acc": 0.65409498,
      "epoch": 1.559748427672956,
      "grad_norm": 2.015625,
      "learning_rate": 7.083333333333334e-05,
      "loss": 1.31055508,
      "step": 310
    },
    {
      "acc": 0.65717721,
      "epoch": 1.610062893081761,
      "grad_norm": 2.078125,
      "learning_rate": 6.979166666666666e-05,
      "loss": 1.27391224,
      "step": 320
    },
    {
      "acc": 0.66044388,
      "epoch": 1.6603773584905661,
      "grad_norm": 2.203125,
      "learning_rate": 6.875e-05,
      "loss": 1.25759382,
      "step": 330
    },
    {
      "acc": 0.64948182,
      "epoch": 1.7106918238993711,
      "grad_norm": 2.125,
      "learning_rate": 6.770833333333334e-05,
      "loss": 1.30579948,
      "step": 340
    },
    {
      "acc": 0.66449022,
      "epoch": 1.7610062893081762,
      "grad_norm": 1.953125,
      "learning_rate": 6.666666666666667e-05,
      "loss": 1.25139179,
      "step": 350
    },
    {
      "acc": 0.65693283,
      "epoch": 1.8113207547169812,
      "grad_norm": 2.109375,
      "learning_rate": 6.562500000000001e-05,
      "loss": 1.27490225,
      "step": 360
    },
    {
      "acc": 0.6558342,
      "epoch": 1.8616352201257862,
      "grad_norm": 2.09375,
      "learning_rate": 6.458333333333334e-05,
      "loss": 1.28073511,
      "step": 370
    },
    {
      "acc": 0.65924997,
      "epoch": 1.9119496855345912,
      "grad_norm": 2.28125,
      "learning_rate": 6.354166666666666e-05,
      "loss": 1.25887384,
      "step": 380
    },
    {
      "acc": 0.66663504,
      "epoch": 1.9622641509433962,
      "grad_norm": 2.109375,
      "learning_rate": 6.25e-05,
      "loss": 1.2434267,
      "step": 390
    },
    {
      "acc": 0.67453065,
      "epoch": 2.0125786163522013,
      "grad_norm": 2.1875,
      "learning_rate": 6.145833333333334e-05,
      "loss": 1.20375528,
      "step": 400
    },
    {
      "acc": 0.69613857,
      "epoch": 2.0628930817610063,
      "grad_norm": 2.796875,
      "learning_rate": 6.041666666666667e-05,
      "loss": 1.10296974,
      "step": 410
    },
    {
      "acc": 0.68712835,
      "epoch": 2.1132075471698113,
      "grad_norm": 2.5,
      "learning_rate": 5.9375e-05,
      "loss": 1.1419445,
      "step": 420
    },
    {
      "acc": 0.69560375,
      "epoch": 2.1635220125786163,
      "grad_norm": 2.765625,
      "learning_rate": 5.833333333333334e-05,
      "loss": 1.11612244,
      "step": 430
    },
    {
      "acc": 0.7032136,
      "epoch": 2.2138364779874213,
      "grad_norm": 2.4375,
      "learning_rate": 5.7291666666666666e-05,
      "loss": 1.08185406,
      "step": 440
    },
    {
      "acc": 0.69179592,
      "epoch": 2.2641509433962264,
      "grad_norm": 2.796875,
      "learning_rate": 5.6250000000000005e-05,
      "loss": 1.13129711,
      "step": 450
    },
    {
      "acc": 0.69704566,
      "epoch": 2.3144654088050314,
      "grad_norm": 2.765625,
      "learning_rate": 5.520833333333334e-05,
      "loss": 1.10817566,
      "step": 460
    },
    {
      "acc": 0.69373083,
      "epoch": 2.3647798742138364,
      "grad_norm": 2.53125,
      "learning_rate": 5.4166666666666664e-05,
      "loss": 1.1209919,
      "step": 470
    },
    {
      "acc": 0.69915276,
      "epoch": 2.4150943396226414,
      "grad_norm": 2.84375,
      "learning_rate": 5.3125000000000004e-05,
      "loss": 1.08133392,
      "step": 480
    },
    {
      "acc": 0.70328555,
      "epoch": 2.4654088050314464,
      "grad_norm": 3.09375,
      "learning_rate": 5.208333333333334e-05,
      "loss": 1.09620447,
      "step": 490
    },
    {
      "acc": 0.69386168,
      "epoch": 2.5157232704402515,
      "grad_norm": 2.625,
      "learning_rate": 5.104166666666666e-05,
      "loss": 1.10145006,
      "step": 500
    },
    {
      "acc": 0.70252738,
      "epoch": 2.5660377358490565,
      "grad_norm": 2.828125,
      "learning_rate": 5e-05,
      "loss": 1.09908714,
      "step": 510
    },
    {
      "acc": 0.69673443,
      "epoch": 2.6163522012578615,
      "grad_norm": 3.0625,
      "learning_rate": 4.8958333333333335e-05,
      "loss": 1.10869875,
      "step": 520
    },
    {
      "acc": 0.69666939,
      "epoch": 2.6666666666666665,
      "grad_norm": 3.15625,
      "learning_rate": 4.791666666666667e-05,
      "loss": 1.10360785,
      "step": 530
    },
    {
      "acc": 0.70942111,
      "epoch": 2.7169811320754715,
      "grad_norm": 3.21875,
      "learning_rate": 4.6875e-05,
      "loss": 1.06053267,
      "step": 540
    },
    {
      "acc": 0.69621105,
      "epoch": 2.767295597484277,
      "grad_norm": 3.21875,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 1.11005697,
      "step": 550
    },
    {
      "acc": 0.70517683,
      "epoch": 2.817610062893082,
      "grad_norm": 3.140625,
      "learning_rate": 4.4791666666666673e-05,
      "loss": 1.07346964,
      "step": 560
    },
    {
      "acc": 0.70620279,
      "epoch": 2.867924528301887,
      "grad_norm": 3.15625,
      "learning_rate": 4.375e-05,
      "loss": 1.07415762,
      "step": 570
    },
    {
      "acc": 0.71239042,
      "epoch": 2.918238993710692,
      "grad_norm": 3.34375,
      "learning_rate": 4.270833333333333e-05,
      "loss": 1.054006,
      "step": 580
    },
    {
      "acc": 0.71305399,
      "epoch": 2.968553459119497,
      "grad_norm": 3.234375,
      "learning_rate": 4.166666666666667e-05,
      "loss": 1.03590345,
      "step": 590
    },
    {
      "acc": 0.71916242,
      "epoch": 3.018867924528302,
      "grad_norm": 3.1875,
      "learning_rate": 4.0625000000000005e-05,
      "loss": 1.02116022,
      "step": 600
    },
    {
      "acc": 0.74165769,
      "epoch": 3.069182389937107,
      "grad_norm": 3.78125,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.93118973,
      "step": 610
    },
    {
      "acc": 0.74692011,
      "epoch": 3.119496855345912,
      "grad_norm": 3.703125,
      "learning_rate": 3.854166666666667e-05,
      "loss": 0.9097496,
      "step": 620
    },
    {
      "acc": 0.74724779,
      "epoch": 3.169811320754717,
      "grad_norm": 3.75,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.91313152,
      "step": 630
    },
    {
      "acc": 0.75153942,
      "epoch": 3.220125786163522,
      "grad_norm": 3.875,
      "learning_rate": 3.6458333333333336e-05,
      "loss": 0.89370852,
      "step": 640
    },
    {
      "acc": 0.75264025,
      "epoch": 3.270440251572327,
      "grad_norm": 3.53125,
      "learning_rate": 3.541666666666667e-05,
      "loss": 0.89566641,
      "step": 650
    },
    {
      "acc": 0.74873405,
      "epoch": 3.3207547169811322,
      "grad_norm": 3.6875,
      "learning_rate": 3.4375e-05,
      "loss": 0.89889536,
      "step": 660
    },
    {
      "acc": 0.75423079,
      "epoch": 3.3710691823899372,
      "grad_norm": 3.84375,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.87628231,
      "step": 670
    },
    {
      "acc": 0.75284452,
      "epoch": 3.4213836477987423,
      "grad_norm": 3.703125,
      "learning_rate": 3.229166666666667e-05,
      "loss": 0.89561214,
      "step": 680
    },
    {
      "acc": 0.75219603,
      "epoch": 3.4716981132075473,
      "grad_norm": 3.703125,
      "learning_rate": 3.125e-05,
      "loss": 0.89204979,
      "step": 690
    },
    {
      "acc": 0.75547881,
      "epoch": 3.5220125786163523,
      "grad_norm": 4.15625,
      "learning_rate": 3.0208333333333334e-05,
      "loss": 0.89397411,
      "step": 700
    },
    {
      "acc": 0.75201845,
      "epoch": 3.5723270440251573,
      "grad_norm": 3.9375,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.89460144,
      "step": 710
    },
    {
      "acc": 0.75330639,
      "epoch": 3.6226415094339623,
      "grad_norm": 4.0,
      "learning_rate": 2.8125000000000003e-05,
      "loss": 0.89762802,
      "step": 720
    },
    {
      "acc": 0.76659098,
      "epoch": 3.6729559748427674,
      "grad_norm": 3.828125,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 0.83859186,
      "step": 730
    },
    {
      "acc": 0.7623024,
      "epoch": 3.7232704402515724,
      "grad_norm": 4.21875,
      "learning_rate": 2.604166666666667e-05,
      "loss": 0.86151867,
      "step": 740
    },
    {
      "acc": 0.75714869,
      "epoch": 3.7735849056603774,
      "grad_norm": 3.890625,
      "learning_rate": 2.5e-05,
      "loss": 0.86721306,
      "step": 750
    },
    {
      "acc": 0.75858154,
      "epoch": 3.8238993710691824,
      "grad_norm": 3.90625,
      "learning_rate": 2.3958333333333334e-05,
      "loss": 0.86395082,
      "step": 760
    },
    {
      "acc": 0.76084099,
      "epoch": 3.8742138364779874,
      "grad_norm": 4.09375,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 0.86485796,
      "step": 770
    },
    {
      "acc": 0.76386518,
      "epoch": 3.9245283018867925,
      "grad_norm": 4.15625,
      "learning_rate": 2.1875e-05,
      "loss": 0.84113998,
      "step": 780
    },
    {
      "acc": 0.76792974,
      "epoch": 3.9748427672955975,
      "grad_norm": 4.0,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.83705759,
      "step": 790
    },
    {
      "acc": 0.77246685,
      "epoch": 4.0251572327044025,
      "grad_norm": 3.765625,
      "learning_rate": 1.9791666666666665e-05,
      "loss": 0.83809834,
      "step": 800
    },
    {
      "acc": 0.79452448,
      "epoch": 4.0754716981132075,
      "grad_norm": 4.40625,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.73750415,
      "step": 810
    },
    {
      "acc": 0.79700141,
      "epoch": 4.1257861635220126,
      "grad_norm": 4.1875,
      "learning_rate": 1.7708333333333335e-05,
      "loss": 0.74755049,
      "step": 820
    },
    {
      "acc": 0.80311651,
      "epoch": 4.176100628930818,
      "grad_norm": 4.375,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.71598339,
      "step": 830
    },
    {
      "acc": 0.79688063,
      "epoch": 4.226415094339623,
      "grad_norm": 4.5,
      "learning_rate": 1.5625e-05,
      "loss": 0.72523293,
      "step": 840
    },
    {
      "acc": 0.79844427,
      "epoch": 4.276729559748428,
      "grad_norm": 4.28125,
      "learning_rate": 1.4583333333333335e-05,
      "loss": 0.74284472,
      "step": 850
    },
    {
      "acc": 0.79428258,
      "epoch": 4.327044025157233,
      "grad_norm": 4.4375,
      "learning_rate": 1.3541666666666666e-05,
      "loss": 0.750599,
      "step": 860
    },
    {
      "acc": 0.79566803,
      "epoch": 4.377358490566038,
      "grad_norm": 4.4375,
      "learning_rate": 1.25e-05,
      "loss": 0.74018383,
      "step": 870
    },
    {
      "acc": 0.80367155,
      "epoch": 4.427672955974843,
      "grad_norm": 4.53125,
      "learning_rate": 1.1458333333333333e-05,
      "loss": 0.7179245,
      "step": 880
    },
    {
      "acc": 0.80155563,
      "epoch": 4.477987421383648,
      "grad_norm": 4.46875,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 0.73799438,
      "step": 890
    },
    {
      "acc": 0.79835653,
      "epoch": 4.528301886792453,
      "grad_norm": 4.125,
      "learning_rate": 9.375000000000001e-06,
      "loss": 0.73876877,
      "step": 900
    },
    {
      "acc": 0.79840956,
      "epoch": 4.578616352201258,
      "grad_norm": 4.34375,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.72253733,
      "step": 910
    },
    {
      "acc": 0.79425635,
      "epoch": 4.628930817610063,
      "grad_norm": 4.09375,
      "learning_rate": 7.2916666666666674e-06,
      "loss": 0.7410614,
      "step": 920
    },
    {
      "acc": 0.79551425,
      "epoch": 4.679245283018868,
      "grad_norm": 4.34375,
      "learning_rate": 6.25e-06,
      "loss": 0.74439735,
      "step": 930
    },
    {
      "acc": 0.80380936,
      "epoch": 4.729559748427673,
      "grad_norm": 4.15625,
      "learning_rate": 5.208333333333334e-06,
      "loss": 0.72431068,
      "step": 940
    },
    {
      "acc": 0.79461656,
      "epoch": 4.779874213836478,
      "grad_norm": 4.15625,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.74081545,
      "step": 950
    },
    {
      "acc": 0.79357677,
      "epoch": 4.830188679245283,
      "grad_norm": 4.46875,
      "learning_rate": 3.125e-06,
      "loss": 0.74980698,
      "step": 960
    },
    {
      "acc": 0.80507946,
      "epoch": 4.880503144654088,
      "grad_norm": 4.34375,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 0.71733956,
      "step": 970
    },
    {
      "acc": 0.80562067,
      "epoch": 4.930817610062893,
      "grad_norm": 4.375,
      "learning_rate": 1.0416666666666667e-06,
      "loss": 0.71620884,
      "step": 980
    },
    {
      "acc": 0.799578,
      "epoch": 4.981132075471698,
      "grad_norm": 4.125,
      "learning_rate": 0.0,
      "loss": 0.74588747,
      "step": 990
    },
    {
      "epoch": 4.981132075471698,
      "eval_acc": 0.7156441023318995,
      "eval_loss": 1.0441640615463257,
      "eval_runtime": 23.7999,
      "eval_samples_per_second": 2.731,
      "eval_steps_per_second": 0.714,
      "step": 990
    }
  ],
  "logging_steps": 10,
  "max_steps": 990,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 1000,
  "total_flos": 6.158400239163146e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
